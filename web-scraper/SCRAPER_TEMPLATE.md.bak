# Scraper Template

When creating a new scraper, use this template:

## HTTP-Based Scraper

```python
# src/scrapers/your_scraper_name.py

from src.worker.registry import registry
from src.worker.scraper import BaseHttpScraper, ScraperError
from src.worker.models import ScrapeResult
from src.shared.models import JobItem
import httpx


@registry.register(
    scraper_id="your-scraper-id",  # Unique ID (e.g., "travis-county-tx")
    name="Your Scraper Name",       # Human-readable name
    version="1.0.0",                # Version for tracking changes
    description="Brief description of what this scraper does"
)
class YourScraperName(BaseHttpScraper):
    """
    Detailed description of the scraper.

    Scrapes data from [website name] for [purpose].

    Required inputs:
        - input_field_1: Description
        - input_field_2: Description

    Returns:
        - field_1: Description
        - field_2: Description
    """

    def validate_inputs(self, job_item: JobItem) -> None:
        """Validate required inputs."""
        required = ['input_field_1', 'input_field_2']
        for field in required:
            if field not in job_item.input:
                raise ScraperError(
                    f"Missing required input: {field}",
                    "invalid_input"
                )

    async def _execute(
        self,
        client: httpx.AsyncClient,
        job_item: JobItem
    ) -> ScrapeResult:
        """
        Main scraping logic.

        Note: httpx exceptions (HTTPStatusError, TimeoutException, RequestError) are
        automatically caught and handled by the base class. Just write the happy path!

        Only raise ScraperError explicitly for custom business logic errors.
        """
        # Extract inputs
        field_1 = job_item.input['input_field_1']
        field_2 = job_item.input['input_field_2']

        # Step 1: Make initial request
        # No need to wrap in try/except - the base class handles httpx errors!
        response = await client.get(
            "https://example.com/search",
            params={"query": field_1}
        )
        response.raise_for_status()

        # Step 2: Parse and extract data
        # ... your parsing logic here ...

        # Optional: Raise custom ScraperError for business logic errors
        # if no_results_found:
        #     from src.shared.models import NonRetryableError
        #     error_type: NonRetryableError = "no_results"
        #     raise ScraperError("No results found", error_type)

        extracted_data = {
            "item_id": job_item.item_id,
            "field_1": "value1",
            "field_2": "value2",
            # ... more fields
        }

        return ScrapeResult(
            html=response.text,
            data=extracted_data,
            screenshot=None
        )
```

## Simple Mock Scraper (for testing)

```python
# src/scrapers/your_mock_scraper.py

from src.worker.registry import registry
from src.worker.models import ScrapeResult
from src.shared.models import JobItem


@registry.register(
    scraper_id="your-mock-scraper",
    name="Your Mock Scraper",
    version="1.0.0",
    description="Mock scraper for testing"
)
class YourMockScraper:
    """Simple mock scraper for testing."""

    async def scrape(self, job_item: JobItem) -> ScrapeResult:
        """Return mock data."""
        mock_html = "<html><body>Mock data</body></html>"

        extracted_data = {
            "item_id": job_item.item_id,
            "mock_field": "mock_value"
        }

        return ScrapeResult(
            html=mock_html,
            data=extracted_data,
            screenshot=None
        )
```

## Usage

Once created, your scraper is automatically registered. Use it in a job:

```json
POST /jobs
{
  "job_id": "job-123",
  "scraper_id": "your-scraper-id",
  "items": [
    {
      "item_id": "item-1",
      "input": {
        "input_field_1": "value1",
        "input_field_2": "value2"
      }
    }
  ]
}
```